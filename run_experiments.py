#!/usr/bin/env python3
"""
CIFAR-10 CNNå®Ÿé¨“è‡ªå‹•å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ§˜ã€…ãªCNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã§å®Ÿé¨“ã‚’å®Ÿè¡Œ
"""

import subprocess
import json
from pathlib import Path
import pandas as pd
import time
import sys
import signal


def run_experiment(params):
    """å˜ä¸€ã®å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    cmd = ["python", "-u", "main.py"]  # -u ãƒ•ãƒ©ã‚°ã§ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
    for key, value in params.items():
        if isinstance(value, bool):
            if value:
                cmd.append(f"--{key}")
        elif isinstance(value, list):
            cmd.append(f"--{key}")
            for v in value:
                cmd.append(str(v))
        else:
            cmd.append(f"--{key}")
            cmd.append(str(value))
    
    print(f"\nå®Ÿè¡Œä¸­: {' '.join(cmd)}")
    print("é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºä¸­...")
    print("-" * 60)
    
    try:
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›ã®ãŸã‚ã«Popenä½¿ç”¨
        process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=0  # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’0ã«è¨­å®š
        )
        
        # å‡ºåŠ›ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º
        output_lines = []
        for line in iter(process.stdout.readline, ''):
            if line:
                print(line.rstrip())  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
                sys.stdout.flush()  # å¼·åˆ¶çš„ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                output_lines.append(line)
        
        return_code = process.wait()
        
        if return_code != 0:
            print(f"\nã‚¨ãƒ©ãƒ¼: å®Ÿé¨“ãŒå¤±æ•—ã—ã¾ã—ãŸ (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code})")
            return None
        
        print(f"\n{'-'*60}")
        print("å®Ÿé¨“å®Œäº†!")
        print(f"{'-'*60}")
        
        return ''.join(output_lines)
        
    except KeyboardInterrupt:
        print("\nå®Ÿé¨“ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
        process.terminate()
        return None
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


def signal_handler(sig, frame):
    """Ctrl+C ã«ã‚ˆã‚‹ä¸­æ–­å‡¦ç†"""
    print(f"\n\nğŸ›‘ å®Ÿé¨“ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    print("ç¾åœ¨ã¾ã§ã®çµæœã¯ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    sys.exit(0)


def main():
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
    signal.signal(signal.SIGINT, signal_handler)
    
    # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
    experiments = [
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªCNNï¼‰
        {
            "experiment_name": "baseline_simple",
            "epochs": 30,
            "conv_layers": 3,
            "filters": [32, 64, 128],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "dense_units": [128]
        },
        
        # 2. ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ5å±¤ï¼‰
        {
            "experiment_name": "deep_5layers",
            "epochs": 40,
            "conv_layers": 5,
            "filters": [32, 64, 128, 256, 512],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 64,
            "dropout_rate": 0.3,
            "use_batch_norm": True,
            "dense_units": [256, 128]
        },
        
        # 3. å¹…åºƒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ•°å¤šã‚ï¼‰
        {
            "experiment_name": "wide_network",
            "epochs": 35,
            "conv_layers": 3,
            "filters": [64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 64,
            "dropout_rate": 0.2,
            "dense_units": [256]
        },
        
        # 4. VGGé¢¨ï¼ˆ3x3ã‚«ãƒ¼ãƒãƒ«ã®ç©ã¿é‡ã­ï¼‰
        {
            "experiment_name": "vgg_style",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [64, 64, 128, 128],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_batch_norm": True,
            "dropout_rate": 0.25,
            "dense_units": [256, 128]
        },
        
        # 5. å¤§ããªã‚«ãƒ¼ãƒãƒ«ï¼ˆ5x5ï¼‰
        {
            "experiment_name": "large_kernel",
            "epochs": 35,
            "conv_layers": 3,
            "filters": [32, 64, 128],
            "kernel_size": 5,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 6. Average Poolingä½¿ç”¨
        {
            "experiment_name": "avg_pooling",
            "epochs": 35,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "average",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_batch_norm": True,
            "dense_units": [128]
        },
        
        # 7. Global Average Poolingä½¿ç”¨
        {
            "experiment_name": "global_avg_pool",
            "epochs": 35,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_global_avg_pool": True,
            "use_batch_norm": True,
            "dense_units": [128]
        },
        
        # 8. å¼·ã„ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        {
            "experiment_name": "strong_augmentation",
            "epochs": 50,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "augmentation_strength": "strong",
            "dropout_rate": 0.3,
            "use_batch_norm": True,
            "dense_units": [256, 128]
        },
        
        # 9. ä¸­ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        {
            "experiment_name": "medium_augmentation",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "augmentation_strength": "medium",
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 10. SGD with momentum
        {
            "experiment_name": "sgd_momentum",
            "epochs": 60,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "sgd",
            "learning_rate": 0.01,
            "batch_size": 64,
            "lr_schedule": "step",
            "use_batch_norm": True,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 11. RMSprop optimizer
        {
            "experiment_name": "rmsprop",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "rmsprop",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_batch_norm": True,
            "dense_units": [128]
        },
        
        # 12. AdamW (with weight decay)
        {
            "experiment_name": "adamw",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adamw",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_batch_norm": True,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 13. ELU activation
        {
            "experiment_name": "elu_activation",
            "epochs": 35,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "elu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 14. Cosine learning rate schedule
        {
            "experiment_name": "cosine_schedule",
            "epochs": 50,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.003,
            "batch_size": 128,
            "lr_schedule": "cosine",
            "use_batch_norm": True,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 15. Warmup + Cosine schedule
        {
            "experiment_name": "warmup_cosine",
            "epochs": 50,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.003,
            "batch_size": 128,
            "lr_schedule": "warmup_cosine",
            "use_batch_norm": True,
            "augmentation_strength": "medium",
            "dropout_rate": 0.25,
            "dense_units": [256, 128]
        },
        
        # 16. å°ãƒãƒƒãƒã‚µã‚¤ã‚º
        {
            "experiment_name": "small_batch",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 32,
            "use_batch_norm": True,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 17. å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º
        {
            "experiment_name": "large_batch",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.002,
            "batch_size": 256,
            "use_batch_norm": True,
            "dense_units": [128]
        },
        
        # 18. ResNeté¢¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
        {
            "experiment_name": "resnet_style",
            "architecture": "resnet",
            "epochs": 50,
            "conv_layers": 4,  # blocksæ•°ã¨ã—ã¦ä½¿ç”¨
            "filters": [64],
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "dropout_rate": 0.2,
            "lr_schedule": "cosine",
            "augmentation_strength": "medium"
        },
        
        # 19. æ··åˆç²¾åº¦å­¦ç¿’
        {
            "experiment_name": "mixed_precision",
            "epochs": 40,
            "conv_layers": 4,
            "filters": [32, 64, 128, 256],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.001,
            "batch_size": 128,
            "use_mixed_precision": True,
            "use_batch_norm": True,
            "dropout_rate": 0.2,
            "dense_units": [128]
        },
        
        # 20. æœ€é©åŒ–å€™è£œï¼ˆç·åˆçš„ãªè¨­å®šï¼‰
        {
            "experiment_name": "optimized_final",
            "epochs": 60,
            "conv_layers": 4,
            "filters": [64, 128, 256, 512],
            "kernel_size": 3,
            "pooling_type": "max",
            "pooling_size": 2,
            "activation": "relu",
            "optimizer": "adam",
            "learning_rate": 0.002,
            "batch_size": 64,
            "lr_schedule": "warmup_cosine",
            "use_batch_norm": True,
            "use_global_avg_pool": True,
            "augmentation_strength": "medium",
            "dropout_rate": 0.25,
            "dense_units": [256, 128],
            "use_mixed_precision": True
        }
    ]
    
    print(f"å®Ÿè¡Œäºˆå®šã®å®Ÿé¨“æ•°: {len(experiments)}")
    print("æ³¨æ„: CIFAR-10ã®å­¦ç¿’ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚å…¨å®Ÿé¨“å®Œäº†ã¾ã§æ•°æ™‚é–“ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    
    # å„å®Ÿé¨“ã‚’å®Ÿè¡Œ
    successful_experiments = 0
    failed_experiments = []
    start_time = time.time()
    
    for i, params in enumerate(experiments, 1):
        print(f"\n{'='*80}")
        print(f"å®Ÿé¨“ {i}/{len(experiments)}: {params['experiment_name']}")
        print(f"{'='*80}")
        
        # å®Ÿé¨“ã®ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print("ä¸»è¦è¨­å®š:")
        if 'architecture' in params:
            print(f"  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {params['architecture']}")
        print(f"  ç•³ã¿è¾¼ã¿å±¤æ•°: {params.get('conv_layers', 'N/A')}")
        print(f"  ãƒ•ã‚£ãƒ«ã‚¿æ•°: {params.get('filters', 'N/A')}")
        print(f"  ã‚¨ãƒãƒƒã‚¯æ•°: {params.get('epochs', 'N/A')}")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {params.get('batch_size', 'N/A')}")
        if params.get('augmentation_strength', 'none') != 'none':
            print(f"  ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: {params['augmentation_strength']}")
        print()
        
        experiment_start = time.time()
        try:
            output = run_experiment(params)
            if output:
                successful_experiments += 1
                experiment_time = time.time() - experiment_start
                print(f"\nâœ… å®Ÿé¨“ {i} æˆåŠŸ (å®Ÿè¡Œæ™‚é–“: {experiment_time/60:.1f}åˆ†)")
            else:
                failed_experiments.append(params['experiment_name'])
                print(f"\nâŒ å®Ÿé¨“ {i} å¤±æ•—")
        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            failed_experiments.append(params['experiment_name'])
        
        # çµŒéæ™‚é–“ã¨æ®‹ã‚Šäºˆæƒ³æ™‚é–“ã‚’è¡¨ç¤º
        elapsed = time.time() - start_time
        if i > 0:
            avg_time_per_exp = elapsed / i
            remaining_time = avg_time_per_exp * (len(experiments) - i)
            print(f"\nğŸ“Š é€²è¡ŒçŠ¶æ³: {i}/{len(experiments)} å®Œäº†")
            print(f"â±ï¸  çµŒéæ™‚é–“: {elapsed/60:.1f}åˆ†")
            print(f"â³ æ®‹ã‚Šäºˆæƒ³æ™‚é–“: {remaining_time/60:.1f}åˆ†")
        
        # GPU/ãƒ¡ãƒ¢ãƒªè§£æ”¾ã®ãŸã‚çŸ­ã„å¾…æ©Ÿ
        time.sleep(3)
    
    # çµæœã®é›†è¨ˆ
    print(f"\n{'='*80}")
    print("å…¨å®Ÿé¨“å®Œäº†ï¼çµæœã‚’é›†è¨ˆä¸­...")
    print(f"{'='*80}")
    
    print(f"\næˆåŠŸã—ãŸå®Ÿé¨“: {successful_experiments}/{len(experiments)}")
    if failed_experiments:
        print(f"å¤±æ•—ã—ãŸå®Ÿé¨“: {', '.join(failed_experiments)}")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§çµæœã‚’è¡¨ç¤º
    summary_file = Path("outputs/experiment_summary.csv")
    if summary_file.exists():
        df = pd.read_csv(summary_file)
        
        # ãƒ†ã‚¹ãƒˆç²¾åº¦ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values("test_accuracy", ascending=False)
        
        print("\n=== Top 10 å®Ÿé¨“çµæœï¼ˆãƒ†ã‚¹ãƒˆç²¾åº¦é †ï¼‰===")
        cols_to_show = ["experiment_name", "test_accuracy", "conv_layers", 
                       "filters", "activation", "optimizer", "batch_size", 
                       "augmentation_strength", "training_time_seconds"]
        
        # è¡¨ç¤ºã™ã‚‹åˆ—ã‚’èª¿æ•´ï¼ˆå­˜åœ¨ã—ãªã„åˆ—ã‚’é™¤å¤–ï¼‰
        cols_to_show = [col for col in cols_to_show if col in df.columns]
        
        print(df_sorted[cols_to_show].head(10).to_string(index=False))
        
        # æœ€è‰¯ã®çµæœã‚’è©³ç´°è¡¨ç¤º
        if len(df_sorted) > 0:
            best = df_sorted.iloc[0]
            print(f"\n=== æœ€è‰¯ã®çµæœ ===")
            print(f"å®Ÿé¨“å: {best['experiment_name']}")
            print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {best['test_accuracy']:.4f}")
            print(f"è¨­å®š:")
            print(f"  - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {best.get('architecture', 'standard')}")
            print(f"  - ç•³ã¿è¾¼ã¿å±¤æ•°: {best['conv_layers']}")
            print(f"  - ãƒ•ã‚£ãƒ«ã‚¿æ•°: {best['filters']}")
            print(f"  - ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º: {best.get('kernel_size', 'N/A')}")
            print(f"  - ãƒ—ãƒ¼ãƒªãƒ³ã‚°: {best.get('pooling_type', 'N/A')}")
            print(f"  - æ´»æ€§åŒ–é–¢æ•°: {best['activation']}")
            print(f"  - æœ€é©åŒ–æ‰‹æ³•: {best['optimizer']}")
            print(f"  - å­¦ç¿’ç‡: {best['learning_rate']}")
            print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {best['batch_size']}")
            print(f"  - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: {best.get('augmentation_strength', 'none')}")
            print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: {best['epochs']}")
            print(f"  - å­¦ç¿’æ™‚é–“: {best['training_time_seconds']:.2f}ç§’")
            
            # åˆ†æç”¨ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
            analysis_summary = {
                "total_experiments": len(df),
                "successful_experiments": successful_experiments,
                "best_accuracy": float(best['test_accuracy']),
                "average_accuracy": float(df['test_accuracy'].mean()),
                "std_accuracy": float(df['test_accuracy'].std()),
                "best_configuration": {
                    "name": best['experiment_name'],
                    "accuracy": float(best['test_accuracy']),
                    "architecture": best.get('architecture', 'standard'),
                    "conv_layers": int(best['conv_layers']),
                    "filters": best['filters'],
                    "optimizer": best['optimizer'],
                    "learning_rate": float(best['learning_rate']),
                    "batch_size": int(best['batch_size']),
                    "augmentation": best.get('augmentation_strength', 'none')
                }
            }
            
            with open("outputs/experiment_summary.json", "w") as f:
                json.dump(analysis_summary, f, indent=2, ensure_ascii=False)
            
            print("\nå®Ÿé¨“ã‚µãƒãƒªãƒ¼ã‚’ outputs/experiment_summary.json ã«ä¿å­˜ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()